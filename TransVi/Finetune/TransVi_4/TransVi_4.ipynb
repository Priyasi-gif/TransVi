{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5f096-c5e3-493b-bb23-165a5f7bfd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fadea2-c42d-44ae-b20a-3c93b62b0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087369b-9050-4289-bfba-2d207e18922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, DataCollatorWithPadding, AdamW, get_scheduler, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e3288-10c6-42cd-a914-2709512aee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_plot(y, title):\n",
    "    sns.countplot(y)\n",
    "    plt.title(f'Class dist in {title} set')\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caadd99b-0fc4-4f78-808b-358b4da10a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(X, y, title):\n",
    "    X_tsne = TSNE(n_components=2, n_iter = 2000 , init='random').fit_transform(X)\n",
    "\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "    plt.title(f'TSNE - {title} data 2D')\n",
    "    plt.legend(title)\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca061193-c65e-4143-84ac-a8b8bedf4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(X, y, title):\n",
    "    # create a PCA object with 2 components\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    # fit the PCA object to X and transform X\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # plot the PCA transform of X colored by y\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0a8c8-5195-47d0-9eb5-82878b26f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HF_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[index]),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[index]),\n",
    "            \"labels\": torch.tensor(self.labels[index]),\n",
    "        }\n",
    "\n",
    "def val_dataset_generator(\n",
    "    tokenizer,\n",
    "    kmer_size,\n",
    "    val_dir,\n",
    "    max_len=512,\n",
    "):\n",
    "    for file in os.listdir(val_dir):\n",
    "        df_test = pd.read_csv(f\"{val_dir}/{file}\")\n",
    "        print(file, len(df_test))\n",
    "        val_kmers, labels_val = [], []\n",
    "\n",
    "        cls = (\n",
    "            \"CLASS\" if \"CLASS\" in df_test.columns else \"Class\"\n",
    "        )  # Ensure to handle both column variations\n",
    "        \n",
    "        for seq, label in zip(df_test[\"SEQ\"], df_test[cls]):\n",
    "            # Filter invalid sequences\n",
    "            if not is_dna_sequence(seq):\n",
    "                continue\n",
    "\n",
    "            # K-mer generation and label adjustment\n",
    "            kmer_seq = return_kmer(seq, K=kmer_size)\n",
    "            val_kmers.append(kmer_seq)\n",
    "            labels_val.append(label - 1)\n",
    "\n",
    "        # Tokenization and padding strategy\n",
    "        val_encodings = tokenizer.batch_encode_plus(\n",
    "            val_kmers,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',  # Ensures all sequences have the same length\n",
    "            truncation=True,  # Truncate sequences longer than max length\n",
    "            return_attention_mask=True, \n",
    "            return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Create dataset\n",
    "        val_dataset = HF_dataset(\n",
    "            val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], labels_val\n",
    "        )\n",
    "        yield val_dataset\n",
    "\n",
    "def return_kmer(seq, K=4):\n",
    "    # Efficient k-mer generation with validation\n",
    "    kmer_list = []\n",
    "    for x in range(len(seq) - K + 1):  # Slide a window of size K\n",
    "        kmer = seq[x : x + K]\n",
    "        if is_dna_sequence(kmer):  # Ensure valid k-mers\n",
    "            kmer_list.append(kmer)\n",
    "\n",
    "    kmer_seq = \" \".join(kmer_list)\n",
    "    return kmer_seq\n",
    "\n",
    "def is_dna_sequence(sequence):\n",
    "    valid_bases = {\"A\", \"C\", \"G\", \"T\"}\n",
    "    return all(base in valid_bases for base in sequence.upper())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b72c8f-54d9-4a28-900f-c60b39776757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face login\n",
    "hf_token = \"hf_PC...r\"\n",
    "login(token=hf_token)\n",
    "\n",
    "# Define model name and K-mer length\n",
    "model_name = 'Priyasi/Pretrain-virusmodel_4'\n",
    "KMER = 4\n",
    "\n",
    "# Load training data\n",
    "df_training = pd.read_csv(\"Trainingdata.csv\")\n",
    "\n",
    "# Placeholder for K-mer function\n",
    "def return_kmer(sequence, K=4):\n",
    "    return [sequence[i:i+K] for i in range(len(sequence) - K + 1)]\n",
    "\n",
    "# Preprocess training data\n",
    "train_kmers, labels_train = [], []\n",
    "for seq, label in zip(df_training[\"SEQ\"], df_training[\"CLASS\"]):\n",
    "    kmer_seq = return_kmer(seq, K=KMER)\n",
    "    train_kmers.append(' '.join(kmer_seq))  # Joining K-mers into a single string\n",
    "    labels_train.append(label - 1)\n",
    "\n",
    "# Define number of classes\n",
    "NUM_CLASSES = len(np.unique(labels_train))\n",
    "\n",
    "# Define model configuration\n",
    "model_config = {\n",
    "    \"model_path\": f\"{model_name}\",  # Use original model name\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "}\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_config[\"model_path\"],\n",
    "    num_labels=NUM_CLASSES  # Number of output labels\n",
    ")\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_config[\"model_path\"], do_lower_case=False, use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "\n",
    "# Encode sequences\n",
    "SEQ_MAX_LEN = 512  # max len of BERT\n",
    "train_encodings = tokenizer.batch_encode_plus(\n",
    "    train_kmers,\n",
    "    max_length=SEQ_MAX_LEN,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Placeholder for HF_dataset\n",
    "# Assuming HF_dataset is a custom Dataset class\n",
    "train_dataset = HF_dataset(\n",
    "    train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], labels_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16018c3-1277-4561-a14b-306b64d9962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(\"Testdata.csv\")  # Load validation data\n",
    "\n",
    "KMER=4\n",
    "val_kmers, labels_val = [], []\n",
    "for seq, label in zip(df_val[\"SEQ\"], df_val[\"CLASS\"]):\n",
    "    kmer_seq = return_kmer(seq, K=KMER)\n",
    "    val_kmers.append(' '.join(kmer_seq))  # Convert K-mers list to a single string\n",
    "    labels_val.append(label - 1)\n",
    "\n",
    "count_plot(labels_val, \"Validation Class Distribution\")\n",
    "\n",
    "# Encode the validation sequences\n",
    "val_encodings = tokenizer.batch_encode_plus(\n",
    "    val_kmers,\n",
    "    max_length=SEQ_MAX_LEN,\n",
    "    padding=True,  # pad to max len\n",
    "    truncation=True,  # truncate to max len\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",  # return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Create the validation dataset\n",
    "val_dataset = HF_dataset(\n",
    "    val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], labels_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ed02b-1b99-4dde-a51a-258173e091f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the results directory if it doesn't exist\n",
    "results_dir = Path(\"./results/virusidentification-4/\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize wandb for logging the training process\n",
    "wandb.init(project=\"DNA_bert\", name=model_config[\"model_path\"])\n",
    "wandb.config.update(model_config)\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 10  # Increasing epochs for better convergence\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # To simulate a larger batch size without requiring more memory\n",
    "LEARNING_RATE = 1e-05  # Try lowering learning rate for fine-tuning\n",
    "\n",
    "# Define training arguments with updates for better accuracy\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_dir / \"check-points\",  # output directory for checkpoints\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.02,  # L2 regularization to reduce overfitting\n",
    "    logging_steps=60,  # log metrics every 60 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Simulate larger batch size\n",
    "    metric_for_best_model=\"accuracy\",  # Track best model based on accuracy\n",
    "    logging_dir=str(results_dir / \"logs\"),  # Log directory\n",
    ")\n",
    "\n",
    "# Initialize the data collator (helps with dynamic padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize the optimizer (AdamW with a lower learning rate for fine-tuning)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize a learning rate scheduler\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=EPOCHS * len(train_dataset) // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Define custom metrics for accuracy, precision, recall, and F1\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0975ce7-55db-42f2-8cb7-78156b539613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Use custom metrics\n",
    "    optimizers=(optimizer, scheduler),  # Pass both optimizer and scheduler\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9c8c5-bbe6-44fb-adbe-e51d674544df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, roc_curve, precision_recall_curve, matthews_corrcoef\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Evaluate the model to get predictions and labels\n",
    "predictions = trainer.predict(val_dataset)\n",
    "\n",
    "# Extract logits and labels\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Convert logits to predicted labels\n",
    "predicted_labels = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfcc2e-b22b-4702-b5bb-04c4dc90157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `labels` and `trainer` are defined earlier in your code\n",
    "# Number of classes\n",
    "num_classes = len(set(labels))\n",
    "\n",
    "# Binarize the labels for multi-class classification (One-vs-Rest approach)\n",
    "binarized_labels = label_binarize(labels, classes=range(num_classes))\n",
    "# Calculate AUC-ROC (One-vs-Rest, weighted average)\n",
    "roc_auc = roc_auc_score(binarized_labels, logits, multi_class=\"ovr\", average=\"weighted\")\n",
    "print(f\"AUC-ROC: {roc_auc}\")\n",
    "\n",
    "# Calculate AUC-PR (One-vs-Rest approach)\n",
    "auc_pr = average_precision_score(binarized_labels, logits, average=\"weighted\")\n",
    "print(f\"AUC-PR: {auc_pr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e31e77-1697-4bbf-af87-9d718a931f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MCC\n",
    "mcc = matthews_corrcoef(labels, predicted_labels)\n",
    "print(f\"MCC: {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600e9f0-3fc6-42e0-9690-bb70d4e781b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Calculate Log Loss\n",
    "log_loss_value = log_loss(binarized_labels, logits)\n",
    "print(f\"Log Loss: {log_loss_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a02843-92d6-47a9-bc17-cc5c6f7158bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa = cohen_kappa_score(labels, predicted_labels)\n",
    "print(f\"Cohen's Kappa: {kappa:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06cc36-111f-4ca4-a589-439825156d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_embedded = tsne.fit_transform(logits)\n",
    "\n",
    "# Plot t-SNE embedding\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(num_classes):\n",
    "    plt.scatter(X_embedded[labels == i, 0], X_embedded[labels == i, 1], label=f'Class {i}')\n",
    "plt.title('t-SNE Embedding of Logits')\n",
    "plt.xlabel('t-SNE Feature 1')\n",
    "plt.ylabel('t-SNE Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('tsne_embedding4.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bbaed-bf43-427c-9858-28d5da41de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(\"./virus_identification_4\")\n",
    "tokenizer.save_pretrained(\"./virus_identificationtoken_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749899c9-0574-44bd-85f6-257e2b11f1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f8fb3-9dd3-4cb3-a1e8-4d28e2fe044f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
